{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3aa7636-39b7-4b0c-9aaa-316ec0a69952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2106f986-0586-430a-962e-086b49092739",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image as Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "67f9aa4b-72b7-47c5-b766-86659c989007",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "picPath='F://eg_pic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6928912-8e57-4360-af07-c1289d58d9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Feature_map(feat):\n",
    "    \"\"\"\n",
    "    第i层的feature map，结构为(C,H,W)→(C,H*W)\n",
    "    \"\"\"\n",
    "    feat = feat.view(feat.shape[0],-1)\n",
    "    \n",
    "    return feat\n",
    "\n",
    "def Loss_content(target, content):\n",
    "    \"\"\"\n",
    "    结构都是(C,H*W)\n",
    "    \"\"\"\n",
    "    It = target.view(target.shape[0],-1)\n",
    "    Ic = content.view(content.shape[0],-1)\n",
    "    loss = 1/2*nn.MSELoss(reduction='sum')(target,content)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e969823f-6a0f-448f-973f-0f74bc91967c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gram(Mat):\n",
    "    \"\"\"\n",
    "    输入（C,H,W）\n",
    "    输出(C,C)\"\"\"\n",
    "    Mat = Feature_map(Mat)\n",
    "    C,M=Mat.shape\n",
    "    G = torch.matmul(Mat,Mat.T)/(4*C*C*M*M)\n",
    "    return G\n",
    "\n",
    "def Loss_style(target, style):\n",
    "    loss = nn.MSELoss(reduction='sum')(Gram(target),Gram(style))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "505bd493-5a72-44a5-9585-4018645fce99",
   "metadata": {},
   "outputs": [],
   "source": [
    "A=torch.Tensor([[1,2,3],\n",
    "                [2,3,4]])\n",
    "B=torch.Tensor([[5,4,1],\n",
    "                [9,7,7]])\n",
    "C=torch.Tensor([[2,3,4],\n",
    "                [2,4,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5edcd8b5-f0da-4db7-ba89-3042184028dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.5000) tensor(49.) \n",
      " tensor(1.4701)\n"
     ]
    }
   ],
   "source": [
    "print(Loss_content(A,C),Loss_content(A,B),\"\\n\",Loss_style(A,B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec4ad6f-7eaa-4815-90bc-99e16ba6337f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b685e16b-c3ae-4315-bb70-d826bfae1c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Flatten_pic(operation, std=0 , mean=0):\n",
    "    if operation == \"style\":\n",
    "        fla_list=[\n",
    "            transforms.Resize(size=[512,512]),\n",
    "            transforms.RandomCrop([256,256]),\n",
    "            transforms.ToTensor()\n",
    "        ]\n",
    "    elif operation == \"content\":\n",
    "        fla_list=[\n",
    "            transforms.Resize(size=[256,256]),\n",
    "            transforms.ToTensor()\n",
    "        ]\n",
    "    elif operation == \"test\":\n",
    "        fla_list=[\n",
    "            transforms.Resize(size=[512,512]),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize()\n",
    "        ]\n",
    "    \n",
    "    return transforms.Compose(fla_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "66a2cbe4-859a-4629-a6ce-290f3fa1671c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    def __init__(self, fPath, transform):\n",
    "        self.tf = transform\n",
    "        self.foldPath = fPath\n",
    "        self.picList = os.listdir(fPath)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "#         img = os.path.join(self.foldPath, self.picList[index])\n",
    "        img = self.foldPath + \"/\" + self.picList[index]\n",
    "        img = Image.open(img).convert('RGB')\n",
    "        img = self.tf(img)\n",
    "        return img\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.picList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60e71ae-8f54-4654-86dc-8a2a1adc75ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_std(pic):\n",
    "    size="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c1da2b93-7aba-440e-ba77-2f6b625e7997",
   "metadata": {},
   "outputs": [],
   "source": [
    "pic=Image.open('F://eg_pic/test1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e00d9284-96bf-407f-8413-20b7fa1765eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pic1=transforms.ToTensor()(pic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c48e1c05-663d-4a49-ace1-4ec0f6ac2fbc",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'PngImageFile' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-a6dab0692bba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'PngImageFile' and 'int'"
     ]
    }
   ],
   "source": [
    "transforms.ToTensor()(pic*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "abf83d0a-259f-4a63-aae1-2f588033f69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pic2=transforms.ToPILImage()(pic1*2)\n",
    "pic2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cfcbbfed-da52-4371-939c-0a3626d16be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf=Flatten_pic(\"content\")\n",
    "dataPic = Preprocessing(picPath, tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "386c2f8c-4445-4448-a1e7-d5c86c09be43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 256, 256])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transforms.ToPILImage()(dataPic[1]).show()\n",
    "dataPic[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "991fcf4f-5e92-4de7-8d8e-88e15fa46db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=dataPic,\n",
    "                                           batch_size=3,\n",
    "                                            drop_last = True ,      \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a2d8c49c-ae2a-4182-83f3-1350a22f08c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 256, 256])\n",
      "torch.Size([3, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "for i in train_loader:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4327edc6-6e81-4be7-bc55-3329ed9726c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
